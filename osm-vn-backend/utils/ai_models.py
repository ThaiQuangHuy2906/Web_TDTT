from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
from typing import List, Optional, Tuple
import re

class AIModels:
    """
    Wrapper for AI models used in OSM-VN backend
    Using lightweight models suitable for HuggingFace free tier
    """
    
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"üîß Initializing AI models on {self.device}...")
        
        # Use lightweight Vietnamese-friendly model
        self.model_name = "google/flan-t5-small"  # T5 is Seq2Seq, NOT CausalLM
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

            # IMPORTANT: Flan-T5 ‚Üí AutoModelForSeq2SeqLM
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name,
                dtype=torch.float16 if self.device == "cuda" else torch.float32
            ).to(self.device)

            # Ensure pad_token exists
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            print(f"‚úÖ Model loaded: {self.model_name}")
            self.loaded = True

        except Exception as e:
            print(f"‚ùå Model loading failed: {e}")
            self.loaded = False
    
    def is_loaded(self) -> bool:
        return self.loaded
    

    # ===================== CHATBOT =====================

    def generate_chat_response(
        self, 
        message: str, 
        history: List[dict] = None,
        location: dict = None
    ) -> str:

        if not self.loaded:
            return "Xin l·ªói, AI chatbot ch∆∞a s·∫µn s√†ng."
        
        context = self._build_context(history, location)
        
        prompt = f"""You are a helpful travel assistant for Vietnam.
User location: {location.get('name', 'Unknown') if location else 'Unknown'}

Previous conversation:
{context}

User: {message}
Assistant:"""
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.replace(prompt, "").strip()
            
            if not response or len(response) < 10:
                response = self._generate_fallback_response(message, location)
            
            return response
        
        except Exception as e:
            print(f"‚ùå Chat generation error: {e}")
            return self._generate_fallback_response(message, location)


    # ===================== POI DESCRIPTION =====================

    def generate_poi_description(
        self,
        poi_name: str,
        poi_type: str,
        location: Optional[str] = None
    ) -> Tuple[str, List[str]]:

        if not self.loaded:
            return self._generate_fallback_description(poi_name, poi_type)
        
        prompt = f"""Generate a short travel description for this place in Vietnam:
Name: {poi_name}
Type: {poi_type}
Location: {location or 'Vietnam'}

Description (2-3 sentences):"""
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", max_length=256, truncation=True)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.8,
                    do_sample=True
                )
            
            description = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            description = description.replace(prompt, "").strip()
            
            highlights = self._extract_highlights(poi_type, description)
            
            if not description or len(description) < 20:
                return self._generate_fallback_description(poi_name, poi_type)
            
            return description, highlights
        
        except Exception as e:
            print(f"‚ùå Description generation error: {e}")
            return self._generate_fallback_description(poi_name, poi_type)


    # ===================== SUGGESTIONS EXTRACTION =====================

    def extract_suggestions(self, text: str) -> List[str]:
        suggestions = []
        
        patterns = [
            r'\d+\.\s*(.+)',
            r'[-‚Ä¢]\s*(.+)',
            r'Try\s+(.+)',
            r'Visit\s+(.+)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.MULTILINE)
            suggestions.extend([m.strip() for m in matches])
        
        return list(dict.fromkeys(suggestions))[:3]
    

    # ===================== HELPERS =====================

    def _build_context(self, history: List[dict], location: dict) -> str:
        if not history:
            return "No previous conversation."
        
        context = ""
        for msg in history[-3:]:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            context += f"{role.capitalize()}: {content}\n"
        
        return context
    

    def _generate_fallback_response(self, message: str, location: dict) -> str:
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["eat", "food", "restaurant", "ƒÉn"]):
            return f"T√¥i g·ª£i √Ω b·∫°n t√¨m c√°c nh√† h√†ng g·∫ßn {location.get('name', 'v·ªã tr√≠ hi·ªán t·∫°i')}. H√£y th·ª≠ b·ªô l·ªçc 'Nh√† h√†ng' ho·∫∑c 'C√† ph√™'!"
        
        elif any(word in message_lower for word in ["visit", "go", "see", "tham quan"]):
            return "B·∫°n c√≥ th·ªÉ kh√°m ph√° c√¥ng vi√™n, b·∫£o t√†ng ho·∫∑c ƒë·ªãa ƒëi·ªÉm n·ªïi b·∫≠t g·∫ßn ƒë√¢y!"
        
        elif any(word in message_lower for word in ["hotel", "stay", "sleep", "kh√°ch s·∫°n"]):
            return "H√£y th·ª≠ t√¨m ki·∫øm 'kh√°ch s·∫°n' ho·∫∑c 'nh√† ngh·ªâ' trong thanh t√¨m ki·∫øm!"
        
        else:
            return f"Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p b·∫°n t√¨m ƒë·ªãa ƒëi·ªÉm ·ªü {location.get('name', 'Vi·ªát Nam')}."


    def _generate_fallback_description(self, poi_name: str, poi_type: str) -> Tuple[str, List[str]]:
        type_map = {
            "restaurant": ("Nh√† h√†ng ph·ª•c v·ª• ·∫©m th·ª±c ƒëa d·∫°ng", ["·∫®m th·ª±c", "Kh√¥ng gian tho·∫£i m√°i"]),
            "cafe": ("Qu√°n c√† ph√™ l√Ω t∆∞·ªüng ƒë·ªÉ th∆∞ gi√£n", ["ƒê·ªì u·ªëng", "Kh√¥ng gian y√™n tƒ©nh"]),
            "park": ("C√¥ng vi√™n xanh m√°t ph√π h·ª£p d·∫°o ch∆°i", ["Thi√™n nhi√™n", "Th∆∞ gi√£n"]),
            "museum": ("B·∫£o t√†ng l∆∞u gi·ªØ di s·∫£n vƒÉn h√≥a", ["L·ªãch s·ª≠", "Gi√°o d·ª•c"]),
        }
        
        desc, highlights = type_map.get(poi_type, (
            f"{poi_name} l√† m·ªôt ƒëi·ªÉm ƒë·∫øn th√∫ v·ªã t·∫°i Vi·ªát Nam.",
            ["ƒê√°ng tham quan"]
        ))
        
        return desc, highlights
    

    def _extract_highlights(self, poi_type: str, description: str) -> List[str]:
        keywords = {
            "restaurant": ["delicious", "authentic", "famous", "traditional"],
            "cafe": ["cozy", "modern", "relaxing", "popular"],
            "park": ["beautiful", "green", "peaceful", "nature"],
            "museum": ["historical", "cultural", "ancient", "artifacts"],
        }
        
        highlights = []
        desc_lower = description.lower()
        
        for keyword in keywords.get(poi_type, []):
            if keyword in desc_lower:
                highlights.append(keyword.capitalize())
        
        return highlights[:3] if highlights else ["ƒê√°ng tham quan"]
